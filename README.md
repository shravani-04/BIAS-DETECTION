Bias Detection and Mitigation in Text and Images
This project focuses on identifying and reducing bias in textual and visual content using machine learning (ML) and deep learning (DL) models. Bias in AI systemsâ€”especially gender and racial biasâ€”can lead to unfair outcomes, and our system aims to make content analysis more fair, inclusive, and responsible.

ğŸ” Problem Statement
In many real-world applicationsâ€”such as news articles, job recommendations, and search enginesâ€”AI models unintentionally learn and repeat biased patterns from historical or skewed data. For example:

Text: Sentences like â€œWomen are too emotional to be leadersâ€ reflect gender stereotypes.

Images: Google Images shows mostly men for the term â€œCEOâ€ and mostly women for â€œnurseâ€.

These examples show the need for a system that can detect and mitigate such bias.

ğŸ’¡ Solution Overview
This project is divided into three key components:

A. Text Bias Detection
We used two techniques to classify news text as biased or unbiased:

TF-IDF + Logistic Regression:

TF-IDF (Term Frequency - Inverse Document Frequency) identifies important words in a sentence.

Logistic Regression is a supervised ML algorithm used to classify a sentence as biased or not.

BERT (Bidirectional Encoder Representations from Transformers):

A transformer-based deep learning model that understands sentence context bidirectionally to improve bias classification.

ğŸ“Œ Example: Input: â€œWomen are too emotional to be leaders.â€
Output: Biased (with confidence score)

B. Text Bias Mitigation
To rewrite biased sentences in a more neutral tone, we used:

T5 (Text-to-Text Transfer Transformer):

A transformer model that reformulates a biased input sentence into a fair and inclusive version.

ğŸ“Œ Example: Input: â€œWomen are too emotional to be leaders.â€
Output: â€œPeople of all genders are equally capable of being leaders.â€

C. Image Bias Analysis
We analyzed image search results for job-related terms (e.g., â€œCEOâ€, â€œteacherâ€, â€œdoctorâ€) using Google Images. The gender and racial distribution of the images was recorded and visualized to highlight stereotypes.

ğŸ“Œ Example:

â€œCEOâ€ results â†’ majority male images

â€œNurseâ€ results â†’ majority female images

We used Python libraries like matplotlib and seaborn to generate visual bias graphs.

ğŸ› ï¸ Technologies Used
Python â€“ Main programming language

scikit-learn â€“ Logistic Regression, TF-IDF

Hugging Face Transformers â€“ BERT and T5 models

Gradio â€“ Web interface for interactive demo

Google Images â€“ Source for image data

matplotlib, seaborn â€“ For image bias visualization

ğŸ“ˆ Results
Text bias detection accuracy: ~85%

T5 model generated effective and neutral rewrites

Image analysis revealed clear gender/professional stereotypes

ğŸ“Œ Example: Biased: â€œMen are better leaders.â€
Rewritten: â€œLeadership is not based on gender.â€

âš ï¸ Challenges
Difficulty in sourcing clean, labeled datasets for bias classification

Ambiguity in defining what counts as â€œbiasedâ€

Ensuring that T5 rewrites preserved original meaning while removing bias
